{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regression Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.zsh\n",
    "$ conda install -c conda-forge xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset into python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "dbunch = fetch_california_housing(as_frame=True)\n",
    "df = dbunch['frame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare raw data for XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode string features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_string_features(df, use_cats=True):\n",
    "    out_df = df.copy()\n",
    "    for feature, feature_type in df.dtypes.items():\n",
    "        if feature_type == 'object':\n",
    "            if use_cats:\n",
    "                out_df[feature] = out_df[feature].astype('category')\n",
    "            else:\n",
    "                from sklearn.preprocessing import LabelEncoder\n",
    "                out_df[feature] = LabelEncoder().fit_transform(out_df[feature].astype('str'))\n",
    "    return out_df\n",
    "\n",
    "df = encode_string_features(df, use_cats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode date and timestamp features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_datetime_features(df, datetime_features, datetime_attributes):\n",
    "    out_df = df.copy()\n",
    "    for datetime_feature in datetime_features:\n",
    "        for datetime_attribute in datetime_attributes:\n",
    "            if datetime_attribute == 'days_since_epoch':\n",
    "                out_df[f'{datetime_feature}_{datetime_attribute}'] = \\\n",
    "                    (out_df[datetime_feature] - pd.Timestamp(year=1970, month=1, day=1)).dt.days\n",
    "            else:\n",
    "                out_df[f'{datetime_feature}_{datetime_attribute}'] = \\\n",
    "                    getattr(out_df[datetime_feature].dt, datetime_attribute)\n",
    "    return out_df\n",
    "\n",
    "datetime_features = [\n",
    "\n",
    "]\n",
    "datetime_attributes = [\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'quarter',\n",
    "    'day_of_year',\n",
    "    'day_of_week',\n",
    "    'days_since_epoch',\n",
    "]\n",
    "\n",
    "df = encode_datetime_features(df, datetime_features, datetime_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the target if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the XGBoost regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dbunch['feature_names']\n",
    "target = dbunch['target_names'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Validation Set\n",
    "def train_test_split_temporal(df, datetime_column, n_test):\n",
    "    idx_sort = np.argsort(df[datetime_column])\n",
    "    idx_train, idx_test = idx_sort[:-n_valid], idx_sort[-n_valid:]\n",
    "    return df.iloc[idx_train, :], df.iloc[idx_test, :]\n",
    "\n",
    "\n",
    "# Random Validation Set\n",
    "def train_test_split_random(df, n_test):\n",
    "    np.random.seed(42)\n",
    "    idx_sort = np.random.permutation(len(df))\n",
    "    idx_train, idx_test = idx_sort[:-n_valid], idx_sort[-n_valid:]\n",
    "    return df.iloc[idx_train, :], df.iloc[idx_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 1000\n",
    "\n",
    "# my_train_test_split = lambda d, n_valid: train_test_split_temporal(d, 'date_column', n_valid)\n",
    "my_train_test_split = lambda d, n_valid: train_test_split_random(d, n_valid)\n",
    "\n",
    "train_df, valid_df = my_train_test_split(df, n_valid)\n",
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using xgboost API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], enable_categorical=True)\n",
    "\n",
    "# default values for important parameters\n",
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "evals_result = {}\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "              verbose_eval=10,\n",
    "              evals_result=evals_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using the sklearn interface"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "reg = xgb.XGBRegressor(n_estimators=num_boost_round, **params)\n",
    "reg.fit(train_df[features], train_df[target], \n",
    "        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n",
    "        verbose=10);\n",
    "        \n",
    "m = reg.get_booster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model and check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "\n",
    "my_eval_metric = root_mean_squared_error\n",
    "my_eval_metric(dvalid.get_label(), m.predict(dvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'train': evals_result['train']['rmse'],\n",
    "    'valid': evals_result['valid']['rmse']\n",
    "}).plot(); plt.xlabel('boosting round'); plt.ylabel('objective');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "feature_importances = pd.Series(m.get_score(importance_type='weight')).sort_values(ascending=False)\n",
    "feature_importances.plot.barh(ax=ax)\n",
    "plt.title('Feature Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'MedInc',\n",
    "     'HouseAge',\n",
    "     'AveRooms',\n",
    "     'AveBedrms',\n",
    "     'Population',\n",
    "     'AveOccup',\n",
    "     'Latitude',\n",
    "     'Longitude'\n",
    "]\n",
    "target = 'MedHouseVal'\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')],verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "#### Drop low-importance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_weight = pd.Series(m.get_score(importance_type='weight')).sort_values(ascending=False)\n",
    "feature_importances_cover = pd.Series(m.get_score(importance_type='cover')).sort_values(ascending=False)\n",
    "feature_importances_gain = pd.Series(m.get_score(importance_type='gain')).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = list(feature_importances_weight[:30].index)\n",
    "# features = list(feature_importances_cover[:35].index)\n",
    "features = list(feature_importances_gain[:8].index)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop one feature at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop each feature one-at-a-time\n",
    "scores = []\n",
    "for i, feature in enumerate(features):\n",
    "    drop_one_features = features[:i] + features[i+1:]\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=train_df[drop_one_features], label=train_df[target], enable_categorical=True)\n",
    "    dvalid = xgb.DMatrix(data=valid_df[drop_one_features], label=valid_df[target], enable_categorical=True)\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': 0.3,\n",
    "        'max_depth': 6,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 1,\n",
    "        'colsample_bynode': 1,\n",
    "        'objective': 'reg:squarederror',\n",
    "    }\n",
    "    num_boost_round = 50\n",
    "\n",
    "    m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "                evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                verbose_eval=False)\n",
    "    score = my_eval_metric(dvalid.get_label(), m.predict(dvalid))\n",
    "    scores.append(score)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'score': scores\n",
    "})\n",
    "results_df.sort_values(by='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'MedInc',\n",
    "     'HouseAge',\n",
    "     'AveRooms',\n",
    "     'AveBedrms',\n",
    "     'Population',\n",
    "     'AveOccup',\n",
    "     'Latitude',\n",
    "     'Longitude'\n",
    "]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], enable_categorical=True)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 1,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "num_boost_round = 50\n",
    "\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the XGBoost hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',}\n",
    "num_boost_round = 50\n",
    "\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')], verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = 50\n",
    "params = {\n",
    "    'learning_rate': 0.3/multiplier,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bynode': 1,\n",
    "    'objective': 'reg:squarederror',}\n",
    "num_boost_round = 50*multiplier\n",
    "\n",
    "m = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "              evals=[(dtrain, 'train'), (dvalid, 'valid')], verbose_eval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval_metric(dvalid.get_label(), m.predict(dvalid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
